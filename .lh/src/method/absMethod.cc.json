{
    "sourceFile": "src/method/absMethod.cc",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 15,
            "patches": [
                {
                    "date": 1733387028836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733387040870,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -561,6 +561,6 @@\n     preLogicalchunkiSize = logicalchunkSize;\n     preuniquechunkSize = uniquechunkSize;\n     preSFTime = SFTime;\n }\n-void Migratory() {};\n-void MLC() {};\n\\ No newline at end of file\n+void AbsMethod::Migratory() {};\n+void AbsMethod::MLC() {};\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733387863284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -562,5 +562,6 @@\n     preuniquechunkSize = uniquechunkSize;\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n-void AbsMethod::MLC() {};\n\\ No newline at end of file\n+void AbsMethod::MLC() {};\n+void AbsMethod::OriLC();\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733387875908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,5 +563,5 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n-void AbsMethod::OriLC();\n\\ No newline at end of file\n+void AbsMethod::OriLC() {};\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733387976040,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,5 +563,47 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n-void AbsMethod::OriLC() {};\n\\ No newline at end of file\n+void AbsMethod::OriLC()\n+{ // 假设原本输入的数据集存储在 input_dataset.dat 文件中\n+    std::ifstream inputFile(\"input_dataset.dat\", std::ios::binary | std::ios::ate);\n+    if (!inputFile.is_open())\n+    {\n+        std::cerr << \"Unable to open input file\";\n+        return;\n+    }\n+\n+    // 获取文件大小\n+    std::streamsize inputSize = inputFile.tellg();\n+    inputFile.seekg(0, std::ios::beg);\n+\n+    // 读取文件内容到缓冲区\n+    std::vector<char> inputBuffer(inputSize);\n+    if (!inputFile.read(inputBuffer.data(), inputSize))\n+    {\n+        std::cerr << \"Error reading input file\";\n+        return;\n+    }\n+    inputFile.close();\n+\n+    // 分配缓冲区用于压缩\n+    int maxCompressedSize = LZ4_compressBound(inputSize);\n+    std::vector<char> compressedBuffer(maxCompressedSize);\n+\n+    // 进行压缩\n+    int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 3);\n+\n+    // 打开输出文件 input_dataset.dat.lz4\n+    std::ofstream outputFile(\"input_dataset.dat.lz4\", std::ios::binary);\n+    if (!outputFile.is_open())\n+    {\n+        std::cerr << \"Unable to open output file\";\n+        return;\n+    }\n+\n+    // 将压缩后的数据写入文件\n+    outputFile.write(compressedBuffer.data(), compressedSize);\n+    outputFile.close();\n+\n+    return;\n+};\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733388001758,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -593,9 +593,9 @@\n     // 进行压缩\n     int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 3);\n \n     // 打开输出文件 input_dataset.dat.lz4\n-    std::ofstream outputFile(\"input_dataset.dat.lz4\", std::ios::binary);\n+    std::ofstream outputFile(\"origin.lz4\", std::ios::binary);\n     if (!outputFile.is_open())\n     {\n         std::cerr << \"Unable to open output file\";\n         return;\n"
                },
                {
                    "date": 1733388050610,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,9 +563,9 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n-void AbsMethod::OriLC()\n+void AbsMethod::OriLC(string inputFilePath)\n { // 假设原本输入的数据集存储在 input_dataset.dat 文件中\n     std::ifstream inputFile(\"input_dataset.dat\", std::ios::binary | std::ios::ate);\n     if (!inputFile.is_open())\n     {\n"
                },
                {
                    "date": 1733388090492,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,14 +563,15 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n-void AbsMethod::OriLC(string inputFilePath)\n-{ // 假设原本输入的数据集存储在 input_dataset.dat 文件中\n-    std::ifstream inputFile(\"input_dataset.dat\", std::ios::binary | std::ios::ate);\n+void AbsMethod::OriLC(const std::string &inputFilePath)\n+{\n+    // 打开输入文件\n+    std::ifstream inputFile(inputFilePath, std::ios::binary | std::ios::ate);\n     if (!inputFile.is_open())\n     {\n-        std::cerr << \"Unable to open input file\";\n+        std::cerr << \"Unable to open input file: \" << inputFilePath << std::endl;\n         return;\n     }\n \n     // 获取文件大小\n@@ -580,9 +581,9 @@\n     // 读取文件内容到缓冲区\n     std::vector<char> inputBuffer(inputSize);\n     if (!inputFile.read(inputBuffer.data(), inputSize))\n     {\n-        std::cerr << \"Error reading input file\";\n+        std::cerr << \"Error reading input file: \" << inputFilePath << std::endl;\n         return;\n     }\n     inputFile.close();\n \n@@ -592,18 +593,23 @@\n \n     // 进行压缩\n     int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 3);\n \n-    // 打开输出文件 input_dataset.dat.lz4\n-    std::ofstream outputFile(\"origin.lz4\", std::ios::binary);\n+    // 构造输出文件路径\n+    std::string outputFilePath = inputFilePath + \".lz4\";\n+\n+    // 打开输出文件\n+    std::ofstream outputFile(outputFilePath, std::ios::binary);\n     if (!outputFile.is_open())\n     {\n-        std::cerr << \"Unable to open output file\";\n+        std::cerr << \"Unable to open output file: \" << outputFilePath << std::endl;\n         return;\n     }\n \n\\ No newline at end of file\n     // 将压缩后的数据写入文件\n     outputFile.write(compressedBuffer.data(), compressedSize);\n     outputFile.close();\n \n+    std::cout << \"Compression successful. Output file: \" << outputFilePath << std::endl;\n+\n     return;\n-};\n+}\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733388138392,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -594,9 +594,9 @@\n     // 进行压缩\n     int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 3);\n \n     // 构造输出文件路径\n-    std::string outputFilePath = inputFilePath + \".lz4\";\n+    std::string outputFilePath = \"ori.lz4\";\n \n     // 打开输出文件\n     std::ofstream outputFile(outputFilePath, std::ios::binary);\n     if (!outputFile.is_open())\n"
                },
                {
                    "date": 1733390071542,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -591,9 +591,9 @@\n     int maxCompressedSize = LZ4_compressBound(inputSize);\n     std::vector<char> compressedBuffer(maxCompressedSize);\n \n     // 进行压缩\n-    int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 3);\n+    int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 9);\n \n     // 构造输出文件路径\n     std::string outputFilePath = \"ori.lz4\";\n \n"
                },
                {
                    "date": 1733390127833,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -591,9 +591,9 @@\n     int maxCompressedSize = LZ4_compressBound(inputSize);\n     std::vector<char> compressedBuffer(maxCompressedSize);\n \n     // 进行压缩\n-    int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 9);\n+    int compressedSize = LZ4_compress_fast(inputBuffer.data(), compressedBuffer.data(), inputSize, maxCompressedSize, 3);\n \n     // 构造输出文件路径\n     std::string outputFilePath = \"ori.lz4\";\n \n"
                },
                {
                    "date": 1733452260051,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,8 +563,11 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n+void AbsMethod::OriGenerate(const std::string &inputFilePath) {\n+\n+};\n void AbsMethod::OriLC(const std::string &inputFilePath)\n {\n     // 打开输入文件\n     std::ifstream inputFile(inputFilePath, std::ios::binary | std::ios::ate);\n"
                },
                {
                    "date": 1733452372941,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,9 +563,9 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n-void AbsMethod::OriGenerate(const std::string &inputFilePath) {\n+void AbsMethod::OriGenerate(std::vector<std::string> readfileList) {\n \n };\n void AbsMethod::OriLC(const std::string &inputFilePath)\n {\n"
                },
                {
                    "date": 1733452447509,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,11 +563,50 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n-void AbsMethod::OriGenerate(std::vector<std::string> readfileList) {\n+void AbsMethod::OriGenerate(std::vector<std::string> readfileList)\n+{\n+    // 打开输出文件 ori\n+    std::ofstream outputFile(\"ori\", std::ios::binary);\n+    if (!outputFile.is_open())\n+    {\n+        std::cerr << \"Unable to open output file: ori\" << std::endl;\n+        return;\n+    }\n \n-};\n+    // 遍历 readfileList 中的每个文件路径\n+    for (const auto &filePath : readfileList)\n+    {\n+        // 打开输入文件\n+        std::ifstream inputFile(filePath, std::ios::binary | std::ios::ate);\n+        if (!inputFile.is_open())\n+        {\n+            std::cerr << \"Unable to open input file: \" << filePath << std::endl;\n+            continue;\n+        }\n+\n+        // 获取文件大小\n+        std::streamsize inputSize = inputFile.tellg();\n+        inputFile.seekg(0, std::ios::beg);\n+\n+        // 读取文件内容到缓冲区\n+        std::vector<char> inputBuffer(inputSize);\n+        if (!inputFile.read(inputBuffer.data(), inputSize))\n+        {\n+            std::cerr << \"Error reading input file: \" << filePath << std::endl;\n+            inputFile.close();\n+            continue;\n+        }\n+        inputFile.close();\n+\n+        // 将缓冲区内容写入输出文件\n+        outputFile.write(inputBuffer.data(), inputSize);\n+    }\n+\n+    outputFile.close();\n+    std::cout << \"Files have been successfully merged into 'ori'.\" << std::endl;\n+}\n void AbsMethod::OriLC(const std::string &inputFilePath)\n {\n     // 打开输入文件\n     std::ifstream inputFile(inputFilePath, std::ios::binary | std::ios::ate);\n"
                },
                {
                    "date": 1733452595204,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -563,9 +563,9 @@\n     preSFTime = SFTime;\n }\n void AbsMethod::Migratory() {};\n void AbsMethod::MLC() {};\n-void AbsMethod::OriGenerate(std::vector<std::string> readfileList)\n+void AbsMethod::OriGenerate(std::vector<std::string> readfileList, int backupNum)\n {\n     // 打开输出文件 ori\n     std::ofstream outputFile(\"ori\", std::ios::binary);\n     if (!outputFile.is_open())\n"
                },
                {
                    "date": 1733452677268,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -573,11 +573,18 @@\n         std::cerr << \"Unable to open output file: ori\" << std::endl;\n         return;\n     }\n \n+    int count = 0;\n+\n     // 遍历 readfileList 中的每个文件路径\n     for (const auto &filePath : readfileList)\n     {\n+        if (count >= backupNum)\n+        {\n+            break;\n+        }\n+\n         // 打开输入文件\n         std::ifstream inputFile(filePath, std::ios::binary | std::ios::ate);\n         if (!inputFile.is_open())\n         {\n@@ -600,8 +607,10 @@\n         inputFile.close();\n \n         // 将缓冲区内容写入输出文件\n         outputFile.write(inputBuffer.data(), inputSize);\n+\n+        count++;\n     }\n \n     outputFile.close();\n     std::cout << \"Files have been successfully merged into 'ori'.\" << std::endl;\n"
                }
            ],
            "date": 1733387028836,
            "name": "Commit-0",
            "content": "#include \"../../include/absmethod.h\"\n\nAbsMethod::AbsMethod()\n{\n    mdCtx = EVP_MD_CTX_new();\n    hashBuf = (uint8_t *)malloc(CHUNK_HASH_SIZE * sizeof(uint8_t));\n}\n\nAbsMethod::~AbsMethod()\n{\n    free(hashBuf);\n}\n\nvoid AbsMethod::SetFilename(string name)\n{\n    filename.assign(name);\n    return;\n}\nvoid AbsMethod::SetTime(std::chrono::time_point<std::chrono::high_resolution_clock> &atime)\n{\n    atime = std::chrono::high_resolution_clock::now();\n}\nbool AbsMethod::compareNat(const std::string &a, const std::string &b)\n{\n    if (a.empty())\n        return true;\n    if (b.empty())\n        return false;\n    if (std::isdigit(a[0]) && !std::isdigit(b[0]))\n        return true;\n    if (!std::isdigit(a[0]) && std::isdigit(b[0]))\n        return false;\n    if (!std::isdigit(a[0]) && !std::isdigit(b[0]))\n    {\n        if (std::toupper(a[0]) == std::toupper(b[0]))\n            return compareNat(a.substr(1), b.substr(1));\n        return (std::toupper(a[0]) < std::toupper(b[0]));\n    }\n\n    // Both strings begin with digit --> parse both numbers\n    std::istringstream issa(a);\n    std::istringstream issb(b);\n    int ia, ib;\n    issa >> ia;\n    issb >> ib;\n    if (ia != ib)\n        return ia < ib;\n\n    // Numbers are the same --> remove numbers and recurse\n    std::string anew, bnew;\n    std::getline(issa, anew);\n    std::getline(issb, bnew);\n    return (compareNat(anew, bnew));\n}\n\nvoid AbsMethod::GenerateHash(EVP_MD_CTX *mdCtx, uint8_t *dataBuffer, const int dataSize, uint8_t *hash)\n{\n    int expectedHashSize = 0;\n\n    if (!EVP_DigestInit_ex(mdCtx, EVP_sha256(), NULL))\n    {\n        fprintf(stderr, \"CryptoTool: Hash init error.\\n\");\n        exit(EXIT_FAILURE);\n    }\n    expectedHashSize = 32;\n\n    if (!EVP_DigestUpdate(mdCtx, dataBuffer, dataSize))\n    {\n        fprintf(stderr, \"CryptoTool: Hash error.\\n\");\n        exit(EXIT_FAILURE);\n    }\n    uint32_t hashSize;\n    if (!EVP_DigestFinal_ex(mdCtx, hash, &hashSize))\n    {\n        fprintf(stderr, \"CryptoTool: Hash error.\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    if (hashSize != expectedHashSize)\n    {\n        fprintf(stderr, \"CryptoTool: Hash size error.\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    EVP_MD_CTX_reset(mdCtx);\n    return;\n}\n\nint AbsMethod::FP_Find(string fp)\n{\n    auto it = FPindex.find(fp);\n    // cout << FPindex.size() << endl;\n    if (it != FPindex.end())\n    {\n        // cout << \"find fp\" << endl;\n        return it->second;\n    }\n    else\n    {\n        return -1;\n    }\n}\n\nbool AbsMethod::FP_Insert(string fp, int chunkid)\n{\n    FPindex[fp] = chunkid;\n    return true;\n}\n\nvoid AbsMethod::GetSF(unsigned char *ptr, EVP_MD_CTX *mdCtx, uint8_t *SF, int dataSize)\n{\n    std::mt19937 gen1, gen2; // 优化\n    std::uniform_int_distribution<uint32_t> full_uint32_t;\n    EVP_MD_CTX *mdCtx_ = mdCtx;\n    int BLOCK_SIZE, WINDOW_SIZE;\n    int SF_NUM, FEATURE_NUM;\n    uint32_t *TRANSPOSE_M;\n    uint32_t *TRANSPOSE_A;\n    int *subchunkIndex;\n    const uint32_t A = 37, MOD = 1000000007;\n    uint64_t Apower = 1;\n    uint32_t *feature;\n    uint64_t *superfeature;\n    gen1 = std::mt19937(922);\n    gen2 = std::mt19937(314159);\n    full_uint32_t = std::uniform_int_distribution<uint32_t>(std::numeric_limits<uint32_t>::min(), std::numeric_limits<uint32_t>::max());\n\n    BLOCK_SIZE = dataSize;\n    WINDOW_SIZE = 48;\n    SF_NUM = FINESSE_SF_NUM; // superfeature的个数\n    FEATURE_NUM = 12;\n    TRANSPOSE_M = new uint32_t[FEATURE_NUM];\n    TRANSPOSE_A = new uint32_t[FEATURE_NUM];\n\n    feature = new uint32_t[FEATURE_NUM];\n    superfeature = new uint64_t[SF_NUM];\n    subchunkIndex = new int[FEATURE_NUM + 1];\n    subchunkIndex[0] = 0;\n    for (int i = 0; i < FEATURE_NUM; ++i)\n    {\n        subchunkIndex[i + 1] = (BLOCK_SIZE * (i + 1)) / FEATURE_NUM;\n    }\n    for (int i = 0; i < FEATURE_NUM; ++i)\n    {\n        TRANSPOSE_M[i] = ((full_uint32_t(gen1) >> 1) << 1) + 1;\n        TRANSPOSE_A[i] = full_uint32_t(gen1);\n    }\n    for (int i = 0; i < WINDOW_SIZE - 1; ++i)\n    {\n        Apower *= A;\n        Apower %= MOD;\n    }\n    for (int i = 0; i < FEATURE_NUM; ++i)\n        feature[i] = 0;\n    for (int i = 0; i < SF_NUM; ++i)\n        superfeature[i] = 0; // 初始化\n\n    for (int i = 0; i < FEATURE_NUM; ++i)\n    {\n        int64_t fp = 0;\n\n        for (int j = subchunkIndex[i]; j < subchunkIndex[i] + WINDOW_SIZE; ++j)\n        {\n            fp *= A;\n            fp += (unsigned char)ptr[j];\n            fp %= MOD;\n        }\n\n        for (int j = subchunkIndex[i]; j < subchunkIndex[i + 1] - WINDOW_SIZE + 1; ++j)\n        {\n            if (fp > feature[i])\n                feature[i] = fp;\n\n            fp -= (ptr[j] * Apower) % MOD;\n            while (fp < 0)\n                fp += MOD;\n            if (j != subchunkIndex[i + 1] - WINDOW_SIZE)\n            {\n                fp *= A;\n                fp += ptr[j + WINDOW_SIZE];\n                fp %= MOD;\n            }\n        }\n    }\n\n    for (int i = 0; i < FEATURE_NUM / SF_NUM; ++i)\n    {\n        std::sort(feature + i * SF_NUM, feature + (i + 1) * SF_NUM);\n    }\n    int offset = 0;\n    for (int i = 0; i < SF_NUM; ++i)\n    {\n        uint64_t temp[FEATURE_NUM / SF_NUM];\n        for (int j = 0; j < FEATURE_NUM / SF_NUM; ++j)\n        {\n            temp[j] = feature[j * SF_NUM + i];\n        }\n        uint8_t *temp3;\n\n        temp3 = (uint8_t *)malloc(FEATURE_NUM / SF_NUM * sizeof(uint64_t));\n\n        memcpy(temp3, temp, FEATURE_NUM / SF_NUM * sizeof(uint64_t));\n\n        uint8_t *temp2;\n        temp2 = (uint8_t *)malloc(CHUNK_HASH_SIZE);\n        this->GenerateHash(mdCtx_, temp3, sizeof(uint64_t) * FEATURE_NUM / SF_NUM, temp2);\n        memcpy(SF + offset, temp2, CHUNK_HASH_SIZE);\n        offset = offset + CHUNK_HASH_SIZE;\n        free(temp2);\n        free(temp3);\n    }\n\n    delete[] TRANSPOSE_M;\n    delete[] TRANSPOSE_A;\n    delete[] feature;\n    delete[] superfeature;\n    delete[] subchunkIndex;\n    return;\n}\n\nint AbsMethod::SF_Find(const char *key, size_t keySize)\n{\n    string keyStr;\n    for (int i = 0; i < FINESSE_SF_NUM; i++)\n    {\n        keyStr.assign(key + i * CHUNK_HASH_SIZE, CHUNK_HASH_SIZE);\n        if (SFindex[i].find(keyStr) != SFindex[i].end())\n        {\n            // cout<<SFindex[i][keyStr].front()<<endl;\n            return SFindex[i][keyStr].back();\n        }\n    }\n    return -1;\n}\n\nbool AbsMethod::SF_Insert(const char *key, size_t keySize, int chunkid)\n{\n    string keyStr;\n    for (int i = 0; i < FINESSE_SF_NUM; i++)\n    {\n        keyStr.assign(key + i * CHUNK_HASH_SIZE, CHUNK_HASH_SIZE);\n        SFindex[i][keyStr].push_back(chunkid);\n    }\n    return true;\n}\n\nuint8_t *AbsMethod::xd3_encode(const uint8_t *targetChunkbuffer, size_t targetChunkbuffer_size, const uint8_t *baseChunkBuffer, size_t baseChunkBuffer_size, size_t *deltaChunkBuffer_size, uint8_t *tmpbuffer)\n{\n    size_t deltachunkSize;\n    int ret = xd3_encode_memory(targetChunkbuffer, targetChunkbuffer_size, baseChunkBuffer, baseChunkBuffer_size, tmpbuffer, &deltachunkSize, CONTAINER_MAX_SIZE * 2, 0);\n    if (ret != 0)\n    {\n        cout << \"delta error\" << endl;\n        const char *errMsg = xd3_strerror(ret);\n        cout << errMsg << endl;\n    }\n    uint8_t *deltaChunkBuffer;\n    deltaChunkBuffer = (uint8_t *)malloc(deltachunkSize);\n    *deltaChunkBuffer_size = deltachunkSize;\n    memcpy(deltaChunkBuffer, tmpbuffer, deltachunkSize);\n    return deltaChunkBuffer;\n}\n\nvoid AbsMethod::PrintChunkInfo(string inputDirpath, int chunkingMethod, int method, int fileNum, int64_t time, double ratio)\n{\n    ofstream out;\n    string fileName = \"./chunkInfoLog.txt\";\n    if (!tool::FileExist(fileName))\n    {\n        out.open(fileName, ios::out);\n        out << \"-----------------INSTRUCTION----------------------\" << endl;\n        out << \"./BiSearch -i \" << inputDirpath << \" -c \" << chunkingMethod << \" -m \" << method << \" -n \" << fileNum << \" -r \" << ratio << endl;\n        out << \"-----------------CHUNK NUM-----------------------\" << endl;\n        out << \"logical chunk num: \" << logicalchunkNum << endl;\n        out << \"unique chunk num: \" << uniquechunkNum << endl;\n        out << \"base chunk num: \" << basechunkNum << endl;\n        out << \"delta chunk num: \" << deltachunkNum << endl;\n        out << \"finesse hit:\" << finessehit << endl;\n        out << \"-----------------CHUNK SIZE-----------------------\" << endl;\n        out << \"logical chunk size: \" << logicalchunkSize << endl;\n        out << \"unique chunk size: \" << uniquechunkSize << endl;\n        out << \"base chunk size: \" << basechunkSize << endl;\n        out << \"delta chunk size: \" << deltachunkSize << endl;\n        out << \"-----------------METRICS-------------------------\" << endl;\n        out << \"Overall Compression Ratio: \" << (double)logicalchunkSize / (double)uniquechunkSize << endl;\n        out << \"DCC: \" << (double)deltachunkNum / (double)uniquechunkNum << endl;\n        out << \"DCR: \" << (double)deltachunkOriSize / (double)deltachunkSize << endl;\n        out << \"DCE: \" << DCESum / (double)deltachunkNum << endl;\n        out << \"-----------------Time------------------------------\" << endl;\n        // out << \"deltaCompressionTime: \" << deltaCompressionTime.count() << \"s\" << endl;\n        out << \"total time: \" << time << \"s\" << endl;\n        out << \"Throughput: \" << (double)logicalchunkSize / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"Reduce data speed: \" << (double)(logicalchunkSize - uniquechunkSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"SF generation time: \" << SFTime.count() << \"s\" << endl;\n        out << \"SF generation throughput: \" << (double)logicalchunkSize / SFTime.count() / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"-----------------OverHead--------------------------\" << endl;\n        // out << \"deltaCompressionTime: \" << deltaCompressionTime.count() << \"s\" << endl;\n        out << \"Index Overhead: \" << (double)(uniquechunkNum * 96 + basechunkNum * 48) / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Recipe Overhead: \" << (double)logicalchunkNum * 8 / 1024 / 1024 << \"MiB\" << endl;\n        out << \"SF number: \" << SFnum << endl;\n        out << \"-----------------Reduct----------------------------\" << endl;\n        out << \"dedup reduct size : \" << DedupReduct << endl;\n        out << \"delta reduct size : \" << DeltaReduct << endl;\n        out << \"local reduct size : \" << LocalReduct << endl;\n        out << \"-----------------END-------------------------------\" << endl;\n    }\n    else\n    {\n        out.open(fileName, ios::app);\n        out << \"-----------------INSTRUCTION----------------------\" << endl;\n        out << \"./BiSearch -i \" << inputDirpath << \" -c \" << chunkingMethod << \" -m \" << method << \" -n \" << fileNum << \" -r \" << ratio << endl;\n        out << \"-----------------CHUNK NUM-----------------------\" << endl;\n        out << \"logical chunk num: \" << logicalchunkNum << endl;\n        out << \"unique chunk num: \" << uniquechunkNum << endl;\n        out << \"base chunk num: \" << basechunkNum << endl;\n        out << \"delta chunk num: \" << deltachunkNum << endl;\n        out << \"finesse hit:\" << finessehit << endl;\n        out << \"-----------------CHUNK SIZE-----------------------\" << endl;\n        out << \"logical chunk size: \" << logicalchunkSize << endl;\n        out << \"unique chunk size: \" << uniquechunkSize << endl;\n        out << \"base chunk size: \" << basechunkSize << endl;\n        out << \"delta chunk size: \" << deltachunkSize << endl;\n        out << \"-----------------METRICS-------------------------\" << endl;\n        out << \"Overall Compression Ratio: \" << (double)logicalchunkSize / (double)uniquechunkSize << endl;\n        out << \"DCC: \" << (double)deltachunkNum / (double)uniquechunkNum << endl;\n        out << \"DCR: \" << (double)deltachunkOriSize / (double)deltachunkSize << endl;\n        out << \"DCE: \" << DCESum / (double)deltachunkNum << endl;\n        out << \"-----------------Time------------------------------\" << endl;\n        // out << \"deltaCompressionTime: \" << deltaCompressionTime.count() << \"s\" << endl;\n        out << \"total time: \" << time << \"s\" << endl;\n        out << \"Throughput: \" << (double)logicalchunkSize / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"Reduce data speed: \" << (double)(logicalchunkSize - uniquechunkSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"SF generation time: \" << SFTime.count() << \"s\" << endl;\n        out << \"SF generation throughput: \" << (double)logicalchunkSize / SFTime.count() / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"-----------------OverHead--------------------------\" << endl;\n        // out << \"deltaCompressionTime: \" << deltaCompressionTime.count() << \"s\" << endl;\n        out << \"Index Overhead: \" << (double)(uniquechunkNum * 96 + basechunkNum * 48) / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Recipe Overhead: \" << (double)logicalchunkNum * 8 / 1024 / 1024 << \"MiB\" << endl;\n        out << \"SF number: \" << SFnum << endl;\n        out << \"-----------------Reduct----------------------------\" << endl;\n        out << \"dedup reduct size : \" << DedupReduct << endl;\n        out << \"delta reduct size : \" << DeltaReduct << endl;\n        out << \"local reduct size : \" << LocalReduct << endl;\n        out << \"-----------------END-------------------------------\" << endl;\n    }\n    out.close();\n    return;\n}\n\nvoid AbsMethod::StatsDelta(Chunk_t &tmpChunk)\n{\n    deltachunkOriSize += tmpChunk.chunkSize;\n    deltachunkSize += tmpChunk.saveSize;\n    deltachunkNum++;\n    DeltaReduct += tmpChunk.chunkSize - tmpChunk.saveSize;\n    DCESum += tmpChunk.chunkSize / tmpChunk.saveSize;\n}\nvoid AbsMethod::StatsDeltaFeature(Chunk_t &tmpChunk)\n{\n    deltachunkOriSize += tmpChunk.chunkSize;\n    deltachunkSize += tmpChunk.saveSize;\n    deltachunkNum++;\n    DeltaReduct += tmpChunk.chunkSize - tmpChunk.saveSize;\n    FeatureReduct += tmpChunk.chunkSize - tmpChunk.saveSize;\n    DCESum += tmpChunk.chunkSize / tmpChunk.saveSize;\n}\n\nvoid AbsMethod::StatsDeltaLocality(Chunk_t &tmpChunk)\n{\n    deltachunkOriSize += tmpChunk.chunkSize;\n    deltachunkSize += tmpChunk.saveSize;\n    deltachunkNum++;\n    DeltaReduct += tmpChunk.chunkSize - tmpChunk.saveSize;\n    LocalityReduct += tmpChunk.chunkSize - tmpChunk.saveSize;\n    DCESum += tmpChunk.chunkSize / tmpChunk.saveSize;\n    LocalityDeltaTime += LocalityDeltaTmp;\n}\n\nvoid AbsMethod::Version_log(double time)\n{\n    cout << \"Version: \" << ads_Version << endl;\n    cout << \"-----------------CHUNK NUM-----------------------\" << endl;\n    cout << \"logical chunk num: \" << logicalchunkNum << endl;\n    cout << \"unique chunk num: \" << uniquechunkNum << endl;\n    cout << \"base chunk num: \" << basechunkNum << endl;\n    cout << \"delta chunk num: \" << deltachunkNum << endl;\n    cout << \"-----------------CHUNK SIZE-----------------------\" << endl;\n    cout << \"logicalchunkSize is \" << logicalchunkSize << endl;\n    cout << \"uniquechunkSize is \" << uniquechunkSize << endl;\n    cout << \"base chunk size: \" << basechunkSize << endl;\n    cout << \"delta chunk size: \" << deltachunkSize << endl;\n    cout << \"-----------------METRICS-------------------------\" << endl;\n    cout << \"Overall Compression Ratio: \" << (double)logicalchunkSize / (double)uniquechunkSize << endl;\n    cout << \"DCC: \" << (double)deltachunkNum / (double)uniquechunkNum << endl;\n    cout << \"DCR: \" << (double)deltachunkOriSize / (double)deltachunkSize << endl;\n    cout << \"DCE: \" << DCESum / (double)deltachunkNum << endl;\n    cout << \"-----------------Time------------------------------\" << endl;\n    // out << \"deltaCompressionTime: \" << deltaCompressionTime.count() << \"s\" << endl;\n    cout << \"Version time: \" << time << \"s\" << endl;\n    cout << \"Throughput: \" << (double)(logicalchunkSize - preLogicalchunkiSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n    cout << \"Reduce data speed: \" << (double)(logicalchunkSize - preLogicalchunkiSize - uniquechunkSize + preuniquechunkSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n    cout << \"SF generation time: \" << SFTime.count() - preSFTime.count() << \"s\" << endl;\n    cout << \"SF generation throughput: \" << (double)(logicalchunkSize - preLogicalchunkiSize) / (SFTime.count() - preSFTime.count()) / 1024 / 1024 << \"MiB/s\" << endl;\n    cout << \"-----------------OverHead--------------------------\" << endl;\n    // out << \"deltaCompressionTime: \" << deltaCompressionTime.count() << \"s\" << endl;\n    cout << \"Index Overhead: \" << (double)(uniquechunkNum * 96 + basechunkNum * 48) / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"Recipe Overhead: \" << (double)logicalchunkNum * 8 / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"SF number: \" << SFnum << endl;\n    cout << \"-----------------END-------------------------------\" << endl;\n\n    preLogicalchunkiSize = logicalchunkSize;\n    preSFTime = SFTime;\n}\n\nvoid AbsMethod::PrintChunkInfo(string inputDirpath, int chunkingMethod, int method, int fileNum, int64_t time, double ratio, double chunktime)\n{\n    ofstream out;\n    string fileName = \"./chunkInfoLog.txt\";\n    if (!tool::FileExist(fileName))\n    {\n        out.open(fileName, ios::out);\n        out << \"-----------------INSTRUCTION----------------------\" << endl;\n        out << \"./BiSearch -i \" << inputDirpath << \" -c \" << chunkingMethod << \" -m \" << method << \" -n \" << fileNum << \" -r \" << ratio << endl;\n        out << \"-----------------CHUNK NUM-----------------------\" << endl;\n        out << \"logical chunk num: \" << logicalchunkNum << endl;\n        out << \"unique chunk num: \" << uniquechunkNum << endl;\n        out << \"base chunk num: \" << basechunkNum << endl;\n        out << \"delta chunk num: \" << deltachunkNum << endl;\n        out << \"finesse hit:\" << finessehit << endl;\n        out << \"-----------------CHUNK SIZE-----------------------\" << endl;\n        out << \"logical chunk size: \" << logicalchunkSize << endl;\n        out << \"unique chunk size: \" << uniquechunkSize << endl;\n        out << \"base chunk size: \" << basechunkSize << endl;\n        out << \"delta chunk size: \" << deltachunkSize << endl;\n        out << \"-----------------METRICS-------------------------\" << endl;\n        out << \"Overall Compression Ratio: \" << (double)logicalchunkSize / (double)uniquechunkSize << endl;\n        out << \"DCC: \" << (double)deltachunkNum / (double)uniquechunkNum << endl;\n        out << \"DCR: \" << (double)deltachunkOriSize / (double)deltachunkSize << endl;\n        out << \"DCE: \" << DCESum / (double)deltachunkNum << endl;\n        out << \"-----------------Time------------------------------\" << endl;\n        out << \"total time: \" << time << \"s\" << endl;\n        out << \"Throughput: \" << (double)logicalchunkSize / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"Reduce data speed: \" << (double)(logicalchunkSize - uniquechunkSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"SF generation time: \" << SFTime.count() << \"s\" << endl;\n        out << \"SF generation throughput: \" << (double)logicalchunkSize / SFTime.count() / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"Chunk Time: \" << chunktime << \"s\" << endl;\n        out << \"Dedup Time: \" << DedupTime.count() << \"s\" << endl;\n        out << \"Locality Match Time: \" << LocalityMatchTime.count() << \"s\" << endl;\n        out << \"Locality Delta Time: \" << LocalityDeltaTime.count() << \"s\" << endl;\n        out << \"Feature Match Time: \" << FeatureMatchTime.count() << \"s\" << endl;\n        out << \"Feature Delta Time: \" << FeatureDeltaTime.count() << \"s\" << endl;\n        out << \"Lz4 Compression Time: \" << lz4CompressionTime.count() << \"s\" << endl;\n        out << \"Delta Compression Time: \" << deltaCompressionTime.count() << \"s\" << endl;\n        out << \"-----------------OverHead--------------------------\" << endl;\n        out << \"Index Overhead: \" << (double)(uniquechunkNum * 96 + basechunkNum * 48) / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Recipe Overhead: \" << (double)logicalchunkNum * 8 / 1024 / 1024 << \"MiB\" << endl;\n        out << \"SF number: \" << SFnum << endl;\n        out << \"-----------------Reduct----------------------------\" << endl;\n        out << \"dedup reduct size : \" << DedupReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"delta reduct size : \" << DeltaReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"local reduct size : \" << LocalReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Feature reduct size: \" << FeatureReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Locality reduct size: \" << LocalityReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"-----------------END-------------------------------\" << endl;\n    }\n    else\n    {\n        out.open(fileName, ios::app);\n        out << \"-----------------INSTRUCTION----------------------\" << endl;\n        out << \"./BiSearch -i \" << inputDirpath << \" -c \" << chunkingMethod << \" -m \" << method << \" -n \" << fileNum << \" -r \" << ratio << endl;\n        out << \"-----------------CHUNK NUM-----------------------\" << endl;\n        out << \"logical chunk num: \" << logicalchunkNum << endl;\n        out << \"unique chunk num: \" << uniquechunkNum << endl;\n        out << \"base chunk num: \" << basechunkNum << endl;\n        out << \"delta chunk num: \" << deltachunkNum << endl;\n        out << \"finesse hit:\" << finessehit << endl;\n        out << \"-----------------CHUNK SIZE-----------------------\" << endl;\n        out << \"logical chunk size: \" << logicalchunkSize << endl;\n        out << \"unique chunk size: \" << uniquechunkSize << endl;\n        out << \"base chunk size: \" << basechunkSize << endl;\n        out << \"delta chunk size: \" << deltachunkSize << endl;\n        out << \"-----------------METRICS-------------------------\" << endl;\n        out << \"Overall Compression Ratio: \" << (double)logicalchunkSize / (double)uniquechunkSize << endl;\n        out << \"DCC: \" << (double)deltachunkNum / (double)uniquechunkNum << endl;\n        out << \"DCR: \" << (double)deltachunkOriSize / (double)deltachunkSize << endl;\n        out << \"DCE: \" << DCESum / (double)deltachunkNum << endl;\n        out << \"-----------------Time------------------------------\" << endl;\n        out << \"total time: \" << time << \"s\" << endl;\n        out << \"Throughput: \" << (double)logicalchunkSize / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"Reduce data speed: \" << (double)(logicalchunkSize - uniquechunkSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"SF generation time: \" << SFTime.count() << \"s\" << endl;\n        out << \"SF generation throughput: \" << (double)logicalchunkSize / SFTime.count() / 1024 / 1024 << \"MiB/s\" << endl;\n        out << \"Chunk Time: \" << chunktime << \"s\" << endl;\n        out << \"Dedup Time: \" << DedupTime.count() << \"s\" << endl;\n        out << \"Locality Match Time: \" << LocalityMatchTime.count() << \"s\" << endl;\n        out << \"Locality Delta Time: \" << LocalityDeltaTime.count() << \"s\" << endl;\n        out << \"Feature Match Time: \" << FeatureMatchTime.count() << \"s\" << endl;\n        out << \"Feature Delta Time: \" << FeatureDeltaTime.count() << \"s\" << endl;\n        out << \"Lz4 Compression Time: \" << lz4CompressionTime.count() << \"s\" << endl;\n        out << \"Delta Compression Time: \" << deltaCompressionTime.count() << \"s\" << endl;\n        out << \"-----------------OverHead--------------------------\" << endl;\n        out << \"Index Overhead: \" << (double)(uniquechunkNum * 96 + basechunkNum * 48) / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Recipe Overhead: \" << (double)logicalchunkNum * 8 / 1024 / 1024 << \"MiB\" << endl;\n        out << \"SF number: \" << SFnum << endl;\n        out << \"-----------------Reduct----------------------------\" << endl;\n        out << \"dedup reduct size : \" << DedupReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"delta reduct size : \" << DeltaReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"local reduct size : \" << LocalReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Feature reduct size: \" << FeatureReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"Locality reduct size: \" << LocalityReduct / 1024 / 1024 << \"MiB\" << endl;\n        out << \"-----------------END-------------------------------\" << endl;\n    }\n    out.close();\n    return;\n}\n\nvoid AbsMethod::Version_log(double time, double chunktime)\n{\n    cout << \"Version: \" << ads_Version << endl;\n    cout << \"-----------------CHUNK NUM-----------------------\" << endl;\n    cout << \"logical chunk num: \" << logicalchunkNum << endl;\n    cout << \"unique chunk num: \" << uniquechunkNum << endl;\n    cout << \"base chunk num: \" << basechunkNum << endl;\n    cout << \"delta chunk num: \" << deltachunkNum << endl;\n    cout << \"-----------------CHUNK SIZE-----------------------\" << endl;\n    cout << \"logicalchunkSize is \" << logicalchunkSize << endl;\n    cout << \"uniquechunkSize is \" << uniquechunkSize << endl;\n    cout << \"base chunk size: \" << basechunkSize << endl;\n    cout << \"delta chunk size: \" << deltachunkSize << endl;\n    cout << \"-----------------METRICS-------------------------\" << endl;\n    cout << \"Overall Compression Ratio: \" << (double)logicalchunkSize / (double)uniquechunkSize << endl;\n    cout << \"DCC: \" << (double)deltachunkNum / (double)uniquechunkNum << endl;\n    cout << \"DCR: \" << (double)deltachunkOriSize / (double)deltachunkSize << endl;\n    cout << \"DCE: \" << DCESum / (double)deltachunkNum << endl;\n    cout << \"-----------------Time------------------------------\" << endl;\n    cout << \"Version time: \" << time << \"s\" << endl;\n    cout << \"Throughput: \" << (double)(logicalchunkSize - preLogicalchunkiSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n    cout << \"Reduce data speed: \" << (double)(logicalchunkSize - preLogicalchunkiSize - uniquechunkSize + preuniquechunkSize) / time / 1024 / 1024 << \"MiB/s\" << endl;\n    cout << \"SF generation time: \" << SFTime.count() - preSFTime.count() << \"s\" << endl;\n    cout << \"SF generation throughput: \" << (double)(logicalchunkSize - preLogicalchunkiSize) / (SFTime.count() - preSFTime.count()) / 1024 / 1024 << \"MiB/s\" << endl;\n    cout << \"Chunk Time: \" << chunktime << \"s\" << endl;\n    cout << \"Dedup Time: \" << DedupTime.count() << \"s\" << endl;\n    cout << \"Locality Match Time: \" << LocalityMatchTime.count() << \"s\" << endl;\n    cout << \"Locality Delta Time: \" << LocalityDeltaTime.count() << \"s\" << endl;\n    cout << \"Feature Match Time: \" << FeatureMatchTime.count() << \"s\" << endl;\n    cout << \"Feature Delta Time: \" << FeatureDeltaTime.count() << \"s\" << endl;\n    cout << \"Lz4 Compression Time: \" << lz4CompressionTime.count() << \"s\" << endl;\n    cout << \"Delta Compression Time: \" << deltaCompressionTime.count() << \"s\" << endl;\n    cout << \"-----------------OVERHEAD--------------------------\" << endl;\n    cout << \"Index Overhead: \" << (double)(uniquechunkNum * 96 + basechunkNum * 48) / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"Recipe Overhead: \" << (double)logicalchunkNum * 8 / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"SF number: \" << SFnum << endl;\n    cout << \"-----------------REDUCT----------------------------\" << endl;\n    cout << \"dedup reduct size : \" << DedupReduct / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"delta reduct size : \" << DeltaReduct / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"local reduct size : \" << LocalReduct / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"Feature reduct size: \" << FeatureReduct / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"Locality reduct size: \" << LocalityReduct / 1024 / 1024 << \"MiB\" << endl;\n    cout << \"-----------------END-------------------------------\" << endl;\n\n    preLogicalchunkiSize = logicalchunkSize;\n    preuniquechunkSize = uniquechunkSize;\n    preSFTime = SFTime;\n}\nvoid Migratory() {};\nvoid MLC() {};"
        }
    ]
}